{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 01:42:40.527685: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-27 01:42:41.278647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 01:42:42.272054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:42.300941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:42.302386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:43.056811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:43.057305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:43.057761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:43.058200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 22237 MB memory:  -> device: 0, name: NVIDIA TITAN RTX, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-06-27 01:42:43.058808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:43.059272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:43.059774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:43.060241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:43.060788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:42:43.061222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 22237 MB memory:  -> device: 0, name: NVIDIA TITAN RTX, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from matplotlib import pyplot as plt \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import os.path\n",
    "from os import path\n",
    "from transformers import TFAutoModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "codebert_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#!pip install transformers==4.20.0\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "use_function_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "data = pd.read_excel('labeled_dataset.xlsx')\n",
    "\n",
    "X_df_all = data[data[\"message\"].notna()]\n",
    "y_df_all = data['comment_group'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCodes():\n",
    "    codes = []\n",
    "    for comment, comment_id, line_no in zip(X[\"message\"], X['comment_id'], X['line_number']):\n",
    "        line_no = int(line_no)\n",
    "        old_code = None\n",
    "\n",
    "        old_folder = code_folder + comment_id + '/Old/'\n",
    "        if(os.path.isdir(old_folder)):\n",
    "            old_code = \"\"\n",
    "            \n",
    "            for filename in listdir(old_folder):\n",
    "                code_lines = read_code_lines(old_folder, filename, line_no)\n",
    "                lines = \"\\n\".join(code_lines)\n",
    "                old_code = old_code + lines\n",
    "        else:\n",
    "            print(\"no old\")\n",
    "\n",
    "        new_code = None\n",
    "        new_folder = code_folder + comment_id + '/New/'\n",
    "        if(os.path.isdir(new_folder)):\n",
    "            new_code = \"\"\n",
    "            for filename in listdir(new_folder):\n",
    "                code_lines = read_code_lines(new_folder, filename, line_no)\n",
    "                lines = \"\\n\".join(code_lines)\n",
    "                new_code = new_code + lines\n",
    "        else:\n",
    "            print(\"no new\")\n",
    "\n",
    "        codes.append((old_code, new_code))\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'codes.pkl'\n",
    "if os.path.exists(file_path):\n",
    "    f = open(file_path, 'rb')\n",
    "    codes = pickle.load(f)\n",
    "else:\n",
    "    codes = getCodes()\n",
    "    f = open(file_path, 'wb')\n",
    "    pickle.dump(codes, f)\n",
    "    # sys.getsizeof(codes)\n",
    "    len(pickle.dumps(codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5000/3622005745.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_df_all.drop(['project', 'revision_id',\n"
     ]
    }
   ],
   "source": [
    "# Drop unused columns\n",
    "X_df_all.drop(['project', 'revision_id',\n",
    "            'parent_revision','branch','file_name','reviewer_id',\n",
    "            'author_id','Downloaded Patch.1', 'URL','change_id','request_id','line_number','patchset_number' ], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DISCUSS</th>\n",
       "      <th>DOCUMENTATION</th>\n",
       "      <th>FALSE POSITIVE</th>\n",
       "      <th>FUNCTION</th>\n",
       "      <th>REFACTORING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1828 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DISCUSS  DOCUMENTATION  FALSE POSITIVE  FUNCTION  REFACTORING\n",
       "0       False          False           False     False         True\n",
       "1       False          False           False      True        False\n",
       "2       False          False           False     False         True\n",
       "3       False          False           False     False         True\n",
       "4       False           True           False     False        False\n",
       "...       ...            ...             ...       ...          ...\n",
       "1823     True          False           False     False        False\n",
       "1824    False           True           False     False        False\n",
       "1825    False          False           False     False         True\n",
       "1826     True          False           False     False        False\n",
       "1827    False          False           False     False         True\n",
       "\n",
       "[1828 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_df = pd.DataFrame(codes, columns=['old_code', 'new_code'])\n",
    "\n",
    "X =pd.concat([X_df_all[\"message\"], code_df[\"old_code\"]], axis=\"columns\")\n",
    "y = pd.get_dummies(X_df_all['comment_group'])\n",
    "\n",
    "Message_numpy=X[\"message\"].to_numpy()\n",
    "\n",
    "pd.get_dummies(X_df_all['comment_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for returning classification report\n",
    "def metrics(model, test_x, y_test, batch_size = 8):\n",
    "    y_preds = model.predict(test_x, batch_size = batch_size)\n",
    "    true_class = tf.argmax( y_test, 1 )\n",
    "    predicted_class = tf.argmax( y_preds, 1 )\n",
    "\n",
    "    print(classification_report(true_class, predicted_class))\n",
    "    cm = confusion_matrix(true_class, predicted_class)\n",
    "    \n",
    "    return classification_report(true_class, predicted_class, output_dict=True), cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatanating code and review comments (message)\n",
    "X_3x = pd.concat([X, code_df[\"new_code\"]], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokenizer, sentence, total_len):\n",
    "    tokens = tokenizer.encode_plus(sentence,max_length=total_len, truncation=True,add_special_tokens=True, padding=\"max_length\")\n",
    "\n",
    "    return tokens[\"input_ids\"], tokens[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_token_slots = 512\n",
    "\n",
    "comment_input_ids = []\n",
    "comment_attention_masks = []\n",
    "old_code_input_ids = []\n",
    "old_code_attention_masks = []\n",
    "new_code_input_ids = []\n",
    "new_code_attention_masks = []\n",
    "for cols, (comment, old_code, new_code) in X_3x.iterrows():\n",
    "    comment = str(comment)\n",
    "    old_code = str(old_code)\n",
    "    new_code = str(new_code)\n",
    "\n",
    "    #input_ids, attention_mask = tokenize_text(bert_tokenizer, comment, total_token_slots)\n",
    "    input_ids, attention_mask = tokenize_text(codebert_tokenizer, comment, total_token_slots)\n",
    "    comment_input_ids.append(input_ids)\n",
    "    comment_attention_masks.append(attention_mask)\n",
    "\n",
    "    input_ids, attention_mask = tokenize_text(codebert_tokenizer, old_code, total_token_slots)\n",
    "    old_code_input_ids.append(input_ids)\n",
    "    old_code_attention_masks.append(attention_mask)\n",
    "\n",
    "    input_ids, attention_mask = tokenize_text(codebert_tokenizer, new_code, total_token_slots)\n",
    "    new_code_input_ids.append(input_ids)\n",
    "    new_code_attention_masks.append(attention_mask)\n",
    "\n",
    "comment_input_ids           = np.array(comment_input_ids)\n",
    "comment_attention_masks     = np.array(comment_attention_masks)\n",
    "old_code_input_ids          = np.array(old_code_input_ids)\n",
    "old_code_attention_masks    = np.array(old_code_attention_masks)\n",
    "new_code_input_ids          = np.array(new_code_input_ids)\n",
    "new_code_attention_masks    = np.array(new_code_attention_masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metrics():\n",
    "    metrics_df_path = 'code_attributes.csv'\n",
    "    metrics_df = pd.read_csv(metrics_df_path)\n",
    "\n",
    "    print(\"metrics columns are eqaul check:\", metrics_df['folderName'].equals(X_df_all['comment_id']))\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics columns are eqaul check: True\n"
     ]
    }
   ],
   "source": [
    "metrics_df = read_metrics()\n",
    "#metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chosen_metrics(metrics_df, chosen_float_metric_columns):\n",
    "    metrics_df = metrics_df.merge(X_df_all[chosen_float_metric_columns],left_index =True, right_index=True)\n",
    "    chosen_float_metric_columns = metrics_df.columns \n",
    "    metrics_df = metrics_df.astype(np.float32)\n",
    "    return metrics_df, chosen_float_metric_columns\n",
    "def drop_error_columns(debugColumns):\n",
    "    metrics_df.drop(debugColumns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_float_metric_columns =['cyclomatic_complexity','comment_loc']\n",
    "\n",
    "metrics_df[\"error\"].replace(np.nan, \"ok\", inplace=True)\n",
    "\n",
    "debugColumns = [\n",
    "        \"folderName\",\n",
    "        \"numOldFiles\",\n",
    "        \"numNewFiles\",\n",
    "        \"error\",\n",
    "        \"isDupe\"\n",
    "    ]\n",
    "\n",
    "drop_error_columns(debugColumns)\n",
    "metrics_df, chosen_metrics_columns = add_chosen_metrics(metrics_df, chosen_float_metric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(clip = True)\n",
    "scaler.fit(metrics_df)\n",
    "metrics_df = scaler.transform(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_input_dict(inputDict):\n",
    "    returnDict = dict()\n",
    "    returnDict[\"input_ids\"] = inputDict[\"input_ids\"][dataIndex]\n",
    "    returnDict[\"token_type_ids\"] = inputDict[\"token_type_ids\"][dataIndex]\n",
    "    returnDict[\"attention_mask\"] = inputDict[\"attention_mask\"][dataIndex]\n",
    "    return returnDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = {\"comment_input_ids\": comment_input_ids ,\n",
    "              \"comment_attention_mask\": comment_attention_masks,\n",
    "                \"code_input_ids\": old_code_input_ids,\n",
    "                \"code_attention_mask\": old_code_attention_masks,\n",
    "             \"metric\" : metrics_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all attributes for modeling together\n",
    "X_all = [comment_input_ids, comment_attention_masks,\n",
    "            old_code_input_ids, old_code_attention_masks,\n",
    "            metrics_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer_1_name = 'lstm_layer_1'\n",
    "lstm_layer_2_name = \"lstm_layer_2\"\n",
    "bert_layer_name = 'tf_roberta_model'\n",
    "def single_bert_layer_hidden_state(prefix):\n",
    "    input_ids_dummy = tf.keras.layers.Input(shape=(total_token_slots,), name=(prefix+'input_ids'), dtype='int64')\n",
    "    mask_dummy = tf.keras.layers.Input(shape=(total_token_slots,), name=(prefix+'attention_mask'), dtype='int64')\n",
    "    codebert = TFAutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "    \n",
    "    embeddings_dummy = codebert(input_ids= input_ids_dummy, attention_mask=mask_dummy)[0]\n",
    "\n",
    "    return input_ids_dummy,mask_dummy,codebert,embeddings_dummy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bert_lstm_text_code():\n",
    "    # two input layers, we ensure layer name variables match to dictionary keys in TF dataset\n",
    "    comment_input_ids_dummy, comment_mask_dummy, comment_codebert, comment_embeddings_dummy = single_bert_layer_hidden_state(\"comment_\")\n",
    "    old_code_input_ids_dummy, old_code_mask_dummy, old_code_codebert, old_code_embeddings_dummy = single_bert_layer_hidden_state(\"code_\")\n",
    "    \n",
    "    comment_x = tf.keras.layers.LSTM(50, dropout=0.3, recurrent_dropout=0.3, name=lstm_layer_1_name)(comment_embeddings_dummy)\n",
    "    old_code_x = tf.keras.layers.LSTM(50, dropout=0.3, recurrent_dropout=0.3, name=lstm_layer_2_name)(old_code_embeddings_dummy)\n",
    "    all_lstm = tf.keras.layers.Concatenate()([comment_x, old_code_x])\n",
    "    \n",
    "    # x = tf.keras.layers.Dense(1024, activation='relu')(embeddings_dummy)\n",
    "    float_metrics_input_dummy = tf.keras.layers.Input(shape=(len(chosen_metrics_columns),), name='metric',dtype='float64')\n",
    "    \n",
    "    x_with_metrics = tf.keras.layers.Concatenate()([all_lstm, float_metrics_input_dummy])\n",
    "    \n",
    "    y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x_with_metrics)\n",
    "\n",
    "    # initialize model\n",
    "    model = tf.keras.Model(inputs=[ comment_input_ids_dummy, comment_mask_dummy,\n",
    "                                   old_code_input_ids_dummy, old_code_mask_dummy,\n",
    "                                    float_metrics_input_dummy], outputs=y)\n",
    "    \n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "    adam_optimizer = Adam(learning_rate=1e-5)\n",
    "    model.compile(loss=loss,optimizer=adam_optimizer,metrics=[acc])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_input_dict(inputDict, dataIndex):\n",
    "    returnDict = dict()\n",
    "    returnDict[\"comment_input_ids\"] = inputDict[\"comment_input_ids\"][dataIndex]\n",
    "    returnDict[\"comment_attention_mask\"] = inputDict[\"comment_attention_mask\"][dataIndex]\n",
    "    returnDict[\"code_input_ids\"] = inputDict[\"code_input_ids\"][dataIndex]\n",
    "    returnDict[\"code_attention_mask\"] = inputDict[\"code_attention_mask\"][dataIndex]\n",
    "    returnDict[\"metric\"] = inputDict[\"metric\"][dataIndex]\n",
    "    #instance_id =inputDict\n",
    "    return returnDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_train_index( length, ratio):\n",
    "    rand_array=np.arange(0,length,1)\n",
    "    train, test =train_test_split(rand_array,test_size=ratio, shuffle=True )\n",
    "    train=np.sort(train)\n",
    "    test=np.sort(test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.values.tolist()\n",
    "\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_with_misclassification(model, test_x, y_test, test_message_comment, batch_size = 8):\n",
    "    \n",
    "    retro_output=[]\n",
    "    \n",
    "    y_preds = model.predict(test_x, batch_size = batch_size)\n",
    "    true_class = tf.argmax( y_test, 1 )\n",
    "    predicted_class = tf.argmax( y_preds, 1 )\n",
    "    \n",
    "    t_class=true_class.numpy()\n",
    "    pred_class=predicted_class.numpy()\n",
    "    \n",
    "    for i in range(0,len(true_class)):\n",
    "        if(t_class[i]!=pred_class[i]):\n",
    "            retro_output.append([test_message_comment[i],t_class[i],pred_class[i]])\n",
    "    \n",
    "    cm = confusion_matrix(true_class, predicted_class)\n",
    "    print(classification_report(true_class, predicted_class))\n",
    "    \n",
    "    return classification_report(true_class, predicted_class, output_dict=True),cm,retro_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 01:43:27.424833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:43:27.425400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:43:27.425886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:43:27.426485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:43:27.426978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:43:27.427460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:43:27.427984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:43:27.428472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-27 01:43:27.429468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22237 MB memory:  -> device: 0, name: NVIDIA TITAN RTX, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "370/370 [==============================] - 461s 1s/step - loss: 1.4535 - categorical_accuracy: 0.4014 - val_loss: 1.2616 - val_categorical_accuracy: 0.5333\n",
      "Epoch 2/4\n",
      "370/370 [==============================] - 444s 1s/step - loss: 1.1255 - categorical_accuracy: 0.5764 - val_loss: 1.1981 - val_categorical_accuracy: 0.5394\n",
      "Epoch 3/4\n",
      "370/370 [==============================] - 442s 1s/step - loss: 0.8591 - categorical_accuracy: 0.6919 - val_loss: 1.1005 - val_categorical_accuracy: 0.5455\n",
      "Epoch 4/4\n",
      "370/370 [==============================] - 443s 1s/step - loss: 0.6180 - categorical_accuracy: 0.7892 - val_loss: 1.1344 - val_categorical_accuracy: 0.5939\n",
      "23/23 [==============================] - 13s 422ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.57      0.59        40\n",
      "           1       0.64      0.83      0.72        30\n",
      "           2       0.50      0.19      0.28        21\n",
      "           3       0.32      0.28      0.30        29\n",
      "           4       0.60      0.70      0.65        63\n",
      "\n",
      "    accuracy                           0.57       183\n",
      "   macro avg       0.53      0.51      0.51       183\n",
      "weighted avg       0.55      0.57      0.55       183\n",
      "\n",
      "Fold:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0', 'tf_roberta_model_3/roberta/pooler/dense/kernel:0', 'tf_roberta_model_3/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0', 'tf_roberta_model_3/roberta/pooler/dense/kernel:0', 'tf_roberta_model_3/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "370/370 [==============================] - 455s 1s/step - loss: 1.4787 - categorical_accuracy: 0.3689 - val_loss: 1.2982 - val_categorical_accuracy: 0.5212\n",
      "Epoch 2/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.2333 - categorical_accuracy: 0.5236 - val_loss: 1.1304 - val_categorical_accuracy: 0.5636\n",
      "Epoch 3/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.0112 - categorical_accuracy: 0.6142 - val_loss: 1.0367 - val_categorical_accuracy: 0.6303\n",
      "Epoch 4/4\n",
      "370/370 [==============================] - 438s 1s/step - loss: 0.7763 - categorical_accuracy: 0.7304 - val_loss: 1.1023 - val_categorical_accuracy: 0.5576\n",
      "23/23 [==============================] - 13s 415ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.67      0.58        43\n",
      "           1       0.64      0.78      0.71        46\n",
      "           2       0.20      0.08      0.12        12\n",
      "           3       0.35      0.39      0.37        18\n",
      "           4       0.73      0.52      0.61        64\n",
      "\n",
      "    accuracy                           0.58       183\n",
      "   macro avg       0.49      0.49      0.48       183\n",
      "weighted avg       0.59      0.58      0.57       183\n",
      "\n",
      "Fold:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_4/roberta/pooler/dense/kernel:0', 'tf_roberta_model_4/roberta/pooler/dense/bias:0', 'tf_roberta_model_5/roberta/pooler/dense/kernel:0', 'tf_roberta_model_5/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_4/roberta/pooler/dense/kernel:0', 'tf_roberta_model_4/roberta/pooler/dense/bias:0', 'tf_roberta_model_5/roberta/pooler/dense/kernel:0', 'tf_roberta_model_5/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "370/370 [==============================] - 455s 1s/step - loss: 1.4946 - categorical_accuracy: 0.3520 - val_loss: 1.3386 - val_categorical_accuracy: 0.5091\n",
      "Epoch 2/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.2984 - categorical_accuracy: 0.4770 - val_loss: 1.1394 - val_categorical_accuracy: 0.5515\n",
      "Epoch 3/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.0548 - categorical_accuracy: 0.5885 - val_loss: 1.1090 - val_categorical_accuracy: 0.5939\n",
      "Epoch 4/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 0.8194 - categorical_accuracy: 0.7169 - val_loss: 1.0774 - val_categorical_accuracy: 0.6061\n",
      "23/23 [==============================] - 13s 412ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.67      0.61        45\n",
      "           1       0.70      0.88      0.78        34\n",
      "           2       0.21      0.20      0.21        15\n",
      "           3       0.54      0.28      0.37        25\n",
      "           4       0.68      0.62      0.65        64\n",
      "\n",
      "    accuracy                           0.60       183\n",
      "   macro avg       0.54      0.53      0.52       183\n",
      "weighted avg       0.59      0.60      0.59       183\n",
      "\n",
      "Fold:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_6/roberta/pooler/dense/kernel:0', 'tf_roberta_model_6/roberta/pooler/dense/bias:0', 'tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_6/roberta/pooler/dense/kernel:0', 'tf_roberta_model_6/roberta/pooler/dense/bias:0', 'tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "370/370 [==============================] - 455s 1s/step - loss: 1.4035 - categorical_accuracy: 0.4176 - val_loss: 1.1991 - val_categorical_accuracy: 0.5455\n",
      "Epoch 2/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.1564 - categorical_accuracy: 0.5534 - val_loss: 1.1075 - val_categorical_accuracy: 0.5758\n",
      "Epoch 3/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 0.9516 - categorical_accuracy: 0.6473 - val_loss: 1.1613 - val_categorical_accuracy: 0.5333\n",
      "Epoch 4/4\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.7289 - categorical_accuracy: 0.7507Restoring model weights from the end of the best epoch: 2.\n",
      "370/370 [==============================] - 439s 1s/step - loss: 0.7289 - categorical_accuracy: 0.7507 - val_loss: 1.1374 - val_categorical_accuracy: 0.6182\n",
      "Epoch 4: early stopping\n",
      "23/23 [==============================] - 13s 416ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.70      0.58        47\n",
      "           1       0.76      0.61      0.68        41\n",
      "           2       0.50      0.06      0.11        16\n",
      "           3       0.00      0.00      0.00        14\n",
      "           4       0.51      0.62      0.56        65\n",
      "\n",
      "    accuracy                           0.54       183\n",
      "   macro avg       0.45      0.40      0.39       183\n",
      "weighted avg       0.52      0.54      0.51       183\n",
      "\n",
      "Fold:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_8/roberta/pooler/dense/kernel:0', 'tf_roberta_model_8/roberta/pooler/dense/bias:0', 'tf_roberta_model_9/roberta/pooler/dense/kernel:0', 'tf_roberta_model_9/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_8/roberta/pooler/dense/kernel:0', 'tf_roberta_model_8/roberta/pooler/dense/bias:0', 'tf_roberta_model_9/roberta/pooler/dense/kernel:0', 'tf_roberta_model_9/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "370/370 [==============================] - 455s 1s/step - loss: 1.4661 - categorical_accuracy: 0.3716 - val_loss: 1.2667 - val_categorical_accuracy: 0.5455\n",
      "Epoch 2/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.2272 - categorical_accuracy: 0.5209 - val_loss: 1.2093 - val_categorical_accuracy: 0.4970\n",
      "Epoch 3/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.0498 - categorical_accuracy: 0.5912 - val_loss: 1.1555 - val_categorical_accuracy: 0.5333\n",
      "Epoch 4/4\n",
      "370/370 [==============================] - 438s 1s/step - loss: 0.8307 - categorical_accuracy: 0.7088 - val_loss: 1.2091 - val_categorical_accuracy: 0.5273\n",
      "23/23 [==============================] - 13s 411ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.67      0.62        43\n",
      "           1       0.74      0.72      0.73        39\n",
      "           2       0.67      0.15      0.25        13\n",
      "           3       0.50      0.17      0.25        24\n",
      "           4       0.58      0.75      0.65        64\n",
      "\n",
      "    accuracy                           0.61       183\n",
      "   macro avg       0.61      0.49      0.50       183\n",
      "weighted avg       0.61      0.61      0.58       183\n",
      "\n",
      "Fold:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_10/roberta/pooler/dense/kernel:0', 'tf_roberta_model_10/roberta/pooler/dense/bias:0', 'tf_roberta_model_11/roberta/pooler/dense/kernel:0', 'tf_roberta_model_11/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_10/roberta/pooler/dense/kernel:0', 'tf_roberta_model_10/roberta/pooler/dense/bias:0', 'tf_roberta_model_11/roberta/pooler/dense/kernel:0', 'tf_roberta_model_11/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "370/370 [==============================] - 456s 1s/step - loss: 1.4520 - categorical_accuracy: 0.3892 - val_loss: 1.3208 - val_categorical_accuracy: 0.5030\n",
      "Epoch 2/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.1907 - categorical_accuracy: 0.5358 - val_loss: 1.1551 - val_categorical_accuracy: 0.5273\n",
      "Epoch 3/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 0.9635 - categorical_accuracy: 0.6439 - val_loss: 1.0818 - val_categorical_accuracy: 0.5515\n",
      "Epoch 4/4\n",
      "370/370 [==============================] - 438s 1s/step - loss: 0.7089 - categorical_accuracy: 0.7615 - val_loss: 1.0202 - val_categorical_accuracy: 0.6061\n",
      "23/23 [==============================] - 13s 414ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75        50\n",
      "           1       0.76      0.76      0.76        34\n",
      "           2       0.40      0.15      0.22        13\n",
      "           3       0.44      0.56      0.49        27\n",
      "           4       0.62      0.61      0.62        59\n",
      "\n",
      "    accuracy                           0.64       183\n",
      "   macro avg       0.59      0.57      0.57       183\n",
      "weighted avg       0.64      0.64      0.63       183\n",
      "\n",
      "Fold:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_12/roberta/pooler/dense/kernel:0', 'tf_roberta_model_12/roberta/pooler/dense/bias:0', 'tf_roberta_model_13/roberta/pooler/dense/kernel:0', 'tf_roberta_model_13/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_12/roberta/pooler/dense/kernel:0', 'tf_roberta_model_12/roberta/pooler/dense/bias:0', 'tf_roberta_model_13/roberta/pooler/dense/kernel:0', 'tf_roberta_model_13/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "370/370 [==============================] - 456s 1s/step - loss: 1.4869 - categorical_accuracy: 0.3527 - val_loss: 1.2150 - val_categorical_accuracy: 0.5455\n",
      "Epoch 2/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.2566 - categorical_accuracy: 0.5095 - val_loss: 1.1182 - val_categorical_accuracy: 0.5758\n",
      "Epoch 3/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.0320 - categorical_accuracy: 0.6047 - val_loss: 0.9757 - val_categorical_accuracy: 0.6364\n",
      "Epoch 4/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 0.8150 - categorical_accuracy: 0.7101 - val_loss: 1.0798 - val_categorical_accuracy: 0.6000\n",
      "23/23 [==============================] - 13s 418ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.77      0.67        47\n",
      "           1       0.70      0.74      0.72        38\n",
      "           2       0.33      0.50      0.40        14\n",
      "           3       0.38      0.27      0.32        22\n",
      "           4       0.67      0.48      0.56        62\n",
      "\n",
      "    accuracy                           0.58       183\n",
      "   macro avg       0.53      0.55      0.53       183\n",
      "weighted avg       0.59      0.58      0.58       183\n",
      "\n",
      "Fold:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_14/roberta/pooler/dense/kernel:0', 'tf_roberta_model_14/roberta/pooler/dense/bias:0', 'tf_roberta_model_15/roberta/pooler/dense/kernel:0', 'tf_roberta_model_15/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_14/roberta/pooler/dense/kernel:0', 'tf_roberta_model_14/roberta/pooler/dense/bias:0', 'tf_roberta_model_15/roberta/pooler/dense/kernel:0', 'tf_roberta_model_15/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "370/370 [==============================] - 456s 1s/step - loss: 1.4770 - categorical_accuracy: 0.3649 - val_loss: 1.3977 - val_categorical_accuracy: 0.5091\n",
      "Epoch 2/4\n",
      "370/370 [==============================] - 439s 1s/step - loss: 1.2510 - categorical_accuracy: 0.5203 - val_loss: 1.2709 - val_categorical_accuracy: 0.4667\n",
      "Epoch 3/4\n",
      "370/370 [==============================] - 440s 1s/step - loss: 1.0125 - categorical_accuracy: 0.6068 - val_loss: 1.1148 - val_categorical_accuracy: 0.5939\n",
      "Epoch 4/4\n",
      "370/370 [==============================] - 440s 1s/step - loss: 0.7553 - categorical_accuracy: 0.7453 - val_loss: 1.0884 - val_categorical_accuracy: 0.6121\n",
      "23/23 [==============================] - 13s 416ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.55      0.57        47\n",
      "           1       0.71      0.80      0.75        40\n",
      "           2       0.00      0.00      0.00        22\n",
      "           3       0.44      0.30      0.36        27\n",
      "           4       0.45      0.72      0.56        47\n",
      "\n",
      "    accuracy                           0.55       183\n",
      "   macro avg       0.44      0.47      0.45       183\n",
      "weighted avg       0.49      0.55      0.51       183\n",
      "\n",
      "Fold:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_16/roberta/pooler/dense/kernel:0', 'tf_roberta_model_16/roberta/pooler/dense/bias:0', 'tf_roberta_model_17/roberta/pooler/dense/kernel:0', 'tf_roberta_model_17/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_16/roberta/pooler/dense/kernel:0', 'tf_roberta_model_16/roberta/pooler/dense/bias:0', 'tf_roberta_model_17/roberta/pooler/dense/kernel:0', 'tf_roberta_model_17/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "371/371 [==============================] - 608s 2s/step - loss: 1.4166 - categorical_accuracy: 0.4031 - val_loss: 1.2991 - val_categorical_accuracy: 0.4545\n",
      "Epoch 2/4\n",
      "371/371 [==============================] - 585s 2s/step - loss: 1.1532 - categorical_accuracy: 0.5449 - val_loss: 1.2441 - val_categorical_accuracy: 0.5091\n",
      "Epoch 3/4\n",
      "371/371 [==============================] - 588s 2s/step - loss: 0.9299 - categorical_accuracy: 0.6462 - val_loss: 1.1565 - val_categorical_accuracy: 0.5455\n",
      "Epoch 4/4\n",
      "371/371 [==============================] - 589s 2s/step - loss: 0.7129 - categorical_accuracy: 0.7610 - val_loss: 1.2024 - val_categorical_accuracy: 0.5455\n",
      "23/23 [==============================] - 13s 418ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68        41\n",
      "           1       0.83      0.81      0.82        47\n",
      "           2       0.50      0.18      0.26        17\n",
      "           3       0.80      0.15      0.26        26\n",
      "           4       0.56      0.82      0.67        51\n",
      "\n",
      "    accuracy                           0.65       182\n",
      "   macro avg       0.66      0.54      0.54       182\n",
      "weighted avg       0.67      0.65      0.61       182\n",
      "\n",
      "Fold:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_18/roberta/pooler/dense/kernel:0', 'tf_roberta_model_18/roberta/pooler/dense/bias:0', 'tf_roberta_model_19/roberta/pooler/dense/kernel:0', 'tf_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_18/roberta/pooler/dense/kernel:0', 'tf_roberta_model_18/roberta/pooler/dense/bias:0', 'tf_roberta_model_19/roberta/pooler/dense/kernel:0', 'tf_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "371/371 [==============================] - 614s 2s/step - loss: 1.4271 - categorical_accuracy: 0.3957 - val_loss: 1.2682 - val_categorical_accuracy: 0.4970\n",
      "Epoch 2/4\n",
      "371/371 [==============================] - 587s 2s/step - loss: 1.1976 - categorical_accuracy: 0.5409 - val_loss: 1.1102 - val_categorical_accuracy: 0.5697\n",
      "Epoch 3/4\n",
      "371/371 [==============================] - 589s 2s/step - loss: 0.9570 - categorical_accuracy: 0.6583 - val_loss: 1.0840 - val_categorical_accuracy: 0.6000\n",
      "Epoch 4/4\n",
      "371/371 [==============================] - 589s 2s/step - loss: 0.7569 - categorical_accuracy: 0.7407 - val_loss: 1.0173 - val_categorical_accuracy: 0.6303\n",
      "23/23 [==============================] - 13s 421ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.60      0.61        42\n",
      "           1       0.60      0.76      0.67        38\n",
      "           2       0.21      0.33      0.26        15\n",
      "           3       0.50      0.43      0.46        28\n",
      "           4       0.67      0.53      0.59        59\n",
      "\n",
      "    accuracy                           0.56       182\n",
      "   macro avg       0.52      0.53      0.52       182\n",
      "weighted avg       0.58      0.56      0.57       182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Code for modeling and calculating model performance\n",
    "\n",
    "precision = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "recall = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "f_score = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "accuracy = 0.0\n",
    "\n",
    "rows, cols = (5, 5)\n",
    "confusion = [[0.0]*cols]*rows\n",
    "error_output=[]\n",
    "count=1\n",
    "\n",
    "random_folding = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "for train_index, test_index in random_folding.split(y):\n",
    "    \n",
    "    print(\"Fold: \",count)\n",
    "    \n",
    "    train_input_set = partition_input_dict(data_input, train_index)\n",
    "    test_input = partition_input_dict(data_input, test_index)\n",
    "    train_output = y[train_index,]\n",
    "    test_output = y[test_index,]\n",
    "    \n",
    "    test_message_comment=Message_numpy[test_index,]\n",
    "    \n",
    "    \n",
    "    training_index, validation_index = generate_test_train_index(train_output.shape[0], 0.1)\n",
    "    model_training_input = partition_input_dict(train_input_set, training_index)\n",
    "    validation_input = partition_input_dict(train_input_set, validation_index)\n",
    "    model_training_output = train_output[training_index]\n",
    "    validation_output = train_output[validation_index]\n",
    "    \n",
    "    \n",
    "    model = get_bert_lstm_text_code()\n",
    "    #model.summary()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode=\"min\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=True,)\n",
    "    \n",
    "    history = model.fit(model_training_input,\n",
    "                        model_training_output, \n",
    "                        batch_size=4,\n",
    "                        epochs=4,\n",
    "                        validation_data=(validation_input, validation_output),\n",
    "                        callbacks=[callback],\n",
    "                        )\n",
    "    \n",
    "    res,confusion_mat, error= results_with_misclassification(model, test_input, test_output, test_message_comment, batch_size = 8)\n",
    "    error_output.append(error)\n",
    "    \n",
    "    confusion += confusion_mat\n",
    "    \n",
    "    precision[0] += res['0']['precision']\n",
    "    precision[1] += res['1']['precision']\n",
    "    precision[2] += res['2']['precision']\n",
    "    precision[3] += res['3']['precision']\n",
    "    precision[4] += res['4']['precision']\n",
    "    \n",
    "    recall[0] += res['0']['recall']\n",
    "    recall[1] += res['1']['recall']\n",
    "    recall[2] += res['2']['recall']\n",
    "    recall[3] += res['3']['recall']\n",
    "    recall[4] += res['4']['recall']\n",
    "    \n",
    "    f_score[0] += res['0']['f1-score']\n",
    "    f_score[1] += res['1']['f1-score']\n",
    "    f_score[2] += res['2']['f1-score']\n",
    "    f_score[3] += res['3']['f1-score']\n",
    "    f_score[4] += res['4']['f1-score']\n",
    "    \n",
    "    accuracy += res['accuracy']\n",
    "    \n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for error analysis\n",
    "debug_file_name = \"error-analysis\"+\".xlsx\"\n",
    "from  functools import reduce\n",
    "debug_info = reduce(lambda x,y: x+y, error_output)\n",
    "\n",
    "debug_df=pd.DataFrame(debug_info, columns=[\"comment\",\"true_class\",\"predicted\"])\n",
    "debug_df.to_excel(debug_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5895060350359798, 0.708204568197881, 0.3522619047619048, 0.42690824534942184, 0.6079775625538881]\n",
      "[0.6723116127927378, 0.7699216306322485, 0.18538057530704588, 0.28184143315177795, 0.6370820168962087]\n",
      "[0.6251164325050699, 0.7339907898768459, 0.2101018834474166, 0.31658896871552394, 0.6106123883939973]\n",
      "0.5875457875457875\n",
      "[[300.  32.  20.  22.  71.]\n",
      " [ 30. 297.   8.   3.  49.]\n",
      " [ 45.  23.  28.  23.  39.]\n",
      " [ 37.  14.  18.  71. 100.]\n",
      " [101.  56.  15.  48. 378.]]\n"
     ]
    }
   ],
   "source": [
    "# Final performance values\n",
    "print([x/10 for x in precision])\n",
    "print([x/10 for x in recall])\n",
    "print([x/10 for x in f_score])\n",
    "print(accuracy/10)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
